# Commands used to obtain the wikipedia dataset
# Author: Tommaso Caselli - January 2017

# download the latest wikipedia dump - our version: en-wiki dump 2017-01-12

wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2

# Extract wikipedia pages only 
# Download the Wikipedia Extractor: git clone https://github.com/attardi/wikiextractor
# Wikipedia Extractor Documentation: http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 
# Wikipedia Extractor runs with python 2.*

# Create folder to store the cleaned wikipedia data
mkdir wikipedia-out 

python WikiExtractor.py -c -o wikipedia-out enwiki-latest-pages-articles.xml.bz2

# Merge all extracted pages from wikipedia into a unique compressed file

cat ./*/*.bz2 > [outFile].bz2

# De-compress the merged wikipedia pages - the sise of the file is about 12GB

bzip2 -d [outFile].bz2 

# Clean the merged wikipedia pages by:
# 1) remove all blank lines
# 2) remove all lines containing tag data from WikiExtractor.py
# 3) all words in lower case
# Notice: the output file is a file which preserved the order in which the WikiExtractor tool has extracted and cleaned the
# wikipedia pages. Size of the cleaned file: 11GB

sed '/^$/d' [outFile] | sed '/^<doc/d' | sed '/^<\/doc>/d' | tr A-Z a-z  > [outFile]_ordered.txt

# Shuffle order. Notice: sentence boundary corresponds to newlines in the cleaned files. This means that
# paragraphs may contain more than one sentence.

perl -MList::Util=shuffle -e 'print shuffle(<STDIN>);' < [outFile]_ordered.txt > [outFile]_shuffled.txt

