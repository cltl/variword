# Commands used to obtain to pre-preprocess the Wikipedia corpus
# Author: Tommaso Caselli - January 2017
# Update 2018-09-13

1) Download the Wikipedia Extractor: git clone https://github.com/attardi/wikiextractor
- Wikipedia Extractor Documentation: http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 
- Wikipedia Extractor runs with python 2.*

2) Place in the Wikipedia Extractor folder the following scripts contained in ./pre_processing_workspace :
- wikipedia-data.bash
- clean_corpus.sh
- remove_duplicate_lines.py
- create_subcorpora.sh
- the MOSES tokenizer folder


3) Run wikipedia-data.bash
- it will download the latest version of Wikipedia (our version: en-wiki dump 2018-08-01 - to fully replicate the experiments)
- it will create the wikipedia-dump and wikipedia-out folder
- it will runs Wikipedia Extractor and merge the results
- Output: the cleaned version of Wikipedia (only Wikipedia pages), ordered alphabetically. File: wikipages_full_ordered.txt

4) Run clean_corpus.sh - input file: wikipages_full_ordered.txt
- coversion to ASCII
- tokenization with MOSES tokenizer
- Punctuation removal and sentence splitting
- duplicate sentence removal
- lowercasing
- shuffle the data

Each step will produce an output file.

5) Run create_subcorpora.sh
- in our version of Wikipedia, we computed the word/sentence ration by means of wc
- wc -l = # of sentences
- wc -w = # of words
- given the word/sentence ration, we extracted head and tail portions of the shuffled and cleaned version 
of Wikipedia to generate the subcorpora



