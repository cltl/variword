Outline of experimental set up and steps taken.

06.09.2018

Currently running:

135M

Next:

last two experiments 200M


Rerun in previous days: 100M and 200M.
200M ran into memory problems for the last two experiments (to be ran later).
We need to run this storing the data under / (using /data as before), but this means running under sudo.
We need to figure out how we can run under sudo and use the right environment (seems like sudo makes python go back to default)

PLAN:
First running all experiments with smaller datasets (stepwise upto 135M tokens), then fix sudo problem and move on to bigger ones.

Done:

creatig 135M:

head -3211500
tail -3211500

75M: first minor results for luong.

creating 75M:

head -1786000
tail -1786000


50M: only luong rare has no real results, others are increasing

20M: as 15M.

15M: most results are actual results (though naturally low); only Luong rare words nothing and analogy extremely low

5M: still some 0 results, but first positive correlations shown, otherwise no problems.

2M: lot of 0 and negative results, otherwise no problem

1M: lot of 0 results, otherwise no problem 

created by:

head -23700
tail -23700

750k: lot of 0 results, otherwise no problem
120K: problem with running SVD initiated experiment: seems like dimensions don't match (why not?)

10.03.2017

120 000 tokens (variae corpus)

head -1250
tail -1250

22.02.2017

750M words corpus:

head -7840000 
tail -7840000

wc:

15.02.2017

Running pmi/svd model, results in error:

/home/ubuntu/variword/hyperwords/representations/embedding.py:22: RuntimeWarning: invalid value encountered in divide
  self.m = self.m / norm[:, np.newaxis]


13.02.2017

400M words done. Changed script so that it writes output to /data/ first (seems to avoid some issues with rights).
About to run 500M, then moving on to somewhat bigger steps (intermediate steps maybe filled later).

500M words corpus:

head -5228000
tail -5228000




06.02.2017

Typo found in get_results_ppmi_svd: neg set to 1 instead of 5. The evaluation script for 300M and smaller corpora must be rerun for svd experiments.

Evaluation 400W; with typo corrected. Following error reported (for all evaluations):
/home/ubuntu/variword/hyperwords/representations/embedding.py:22: RuntimeWarning: invalid value encountered in divide
  self.m = self.m / norm[:, np.newaxis]


05.02.2017


For 300M, we used:

head -3137000
tail -3137000

For 400M:

head -4182000
tail -4182000


04.02.2017

Ran experiment recommended with 200M tokens; similar differences observed; clear higher results for each method.

tail -2091000
head -2091000



03.02.2017

Moved previous experiments to VM and launched experiment with ±136M tokens and recommended settings. First incoming results are slightly higher.
Next steps for smaller scale experiments, once this is done (continuing with recommended settings):

200M tokens
300M tokens
500M tokens

In the meantime, an experiment with the full corpus is also launched (pairs2count script crashed on previous trials, issues with tmp/ dir and access rights solved now).


02.02.2017

The full set of experiments with vanille settings took approximately 14 hours to run. The recommended settings are significantly faster (around 2 hours without the svd initiated experiences), we will thus first run experiments with recommended settings only and then see which experiments we also want to run with 'vanilla' settings. 

Experiments with svd initialization crashed: the word2vecf implementation could not handle non-values (which sometimes occur when the initiation is too good). This has now been corrected. The word2vec svd initiated experiments were run separately after the others had finished.

The next steps are: 

1. running experiments with the full corpus on an alternative VM, then start to reduce this corpus.
2. in parallel; run experiments with increasingly large corpora on the smaller VM starting with ±135M words (comparable to the news corpus).

We created a corpus of 135835265 tokens (comparable to news corpus in size). Taking:
head -1420000
tail -1420000


31.01.2017

Results were inspected yesterday and the outcome of the models trained on 103M words randomly selected by wikipedia perform drastically worst than the 130M news corpus.
We found out the cleaning step was skipped and reran the vanille experiment with the cleaned data. Due to a typo in the script, the experiments with the reversed example order did not run.
We ran those afterwards and corrected the bug in the script. Results with the clean corpus were similar (slightly better for most models, slightly worst for the best, overall still far below the news).

Today, we are updating the script that runs experiments with corrected settings so that it also creates models that walk to the corpus in reversed order. We rerun the recommended setting on the clean corpus.
The next step will be increasing the corpus. We will start by a corpus of 130M words (approximating the news corpus in size) and will then move on to larger corpora.

Double checked corpus: it had a little over 103M characters and only 16.3M tokens. Recreated corpus of 100M characters. Taking:
head -1045500 
tail -1045500
of the shuffled wikipedia corpus. This results in slightly over 100M tokens after cleaning.


29.01.2017

The vanilla settings on the first corpus finished. Yesterday, we ran the same experiment using the recommended settings.
These are:

win=2, dyn=dirty, sub=1e-5, neg=5, cds=0.75, w+c=w only, eig=0, nrm=none, svd-dimension=500, sgns-iterations=50.
The command used for both vanilla and recommended settings are stored under the run_ directory of the experiment in 'setup.txt'.
We're adapting the run scripts now to always print the setup with the results for future experiments.

We did not include training the SGNS models with flipped orders of examples yet. These experiments will be run now using the same initiations of the earlier models. We will then adapt the scripts so that these experiments are always run after the ones in original order.

The experiments described above have been carried out.

27.01.2017

Commands for subcorpus creation from the shuffles wikipedia file (1.1M token):
(N.B. for the wikipedia_shuffled file the ration token/sentence is 48token/sentence)
1) head -170000 [wikipedia_shuffled] > out_head
2) tail -170000 [wikipedia_shuffled] > out_tail
3) cat out_head out_tail > wikipedia_shuffled_1.1M


First results on the subcorpus have been created with Levy's default settings. We consider this an exploratory study.

Our current hypotheses are:

1. when trained on the full wikipedia set, we may still observe minor impact of initiation and example order for word to vec, but we expect that the model has converged by then; in the sense that there are no radically different sense representations to be found.
2. with a corpussize comparable to the BNC, we expect to observe differences that can lead to word2vec sometimes being better and sometimes worse than count methods. We also expect that some words have noticeably different representations between models.
3. we expect that the corpus sizes that are used for diachronic studies will lead to highly unstable results (based on both our preliminary observations on the ±136M token dataset and Hellrich and Hahn's observations)

Next steps:

1. We create models with the vanille settings defined in Levy et al. 2015 for a corpus of ±103M words.

Settings: win=2, dyn=none, sub=none, neg=1, cds=1, w+c=w only, eig=0, nrm=none, svd-dimension=500, sgns-iterations=50

Models:
- ppmi
- svd
- 3 random initiation settings starting with examples on top
- initiating with svd starting with example on top
- the same 3 random initiation settings starting with examples on bottom
- initiating with svd starting with example on bottom


We then run the same experiment for the recommended word2vec settings.





Scripts description:

1. creating pairs and counts from clean corpus:

clean_data_2_pairs_and_counts.sh cleancorpus outputdir window

creates in the outputdir:
- pairs: all word pairs in the defined window
- counts.words.vocab: counts of each pair

Notes:
a) the only option given is the window, all others are defaults of hyperwords scripts.
b) the counts are created by an alternative implementations of hyperwords/pairs2counts.py called pairs2counts_only_words.py. 
This scripts only outputs the counts of words in a file. The counts of the context vocabulary are identical to the words vocabulary.
We only output one to save space.


